---
title: "Project"
author: "Trude Almestrand"
date: "8 11 2020"
output:
  html_document:
    code_folding: hide
    warning: FALSE
    message: FALSE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Project

## Abstract

Machine learning is a form of data-analysis that automates the model building progress given a dataset. A multitude of modelling algorithms can be used with many different classes of methods. Machine learning has proven quite useful in biological processes and in this paper an attempt was made to differentiate cancer types using RNA-Seq data from The Cancer Genome Atlas Program. The results showed a good separation between namely ovary, skin and testis cancer types, but it proved difficult to separate colon and rectum cancer data. Feature selection was then used to reduce the data for colon and rectal cancer. An attempt to separate the two cancer types using a combination of linear an non-linear classification machine learning methods proved difficult with an accuracy spanning between 0.46 - 0.655. A suggestion to use different machine learning method for this classification such as deeplearning was made. 

## Introduction

The Cancer Genome Atlas Program us a database consisting of 2.5 petabytes of genomic, epigenomic, transcriptomic and proteomic data (NCI, n.y). This data is publicly available for anyone. Using RNA-Seq data one could attempt to differentiate between the different cancer types available from TCGA. TCGA has lately become a "training" reference to apply machine learning algorithms with over 11 000 tumors across 33 major cancer types (NCI, n.y). Machine learning has in the later years facilitated the development of cancer classifier(Lim, 2019). In order to test a machine-learning method one could attempt to see if the method differentiates two cancer-data unrelated to another such as lung-cancer and ovarian cancer, or attempt to differentiate between two cancers difficult to differentiate. A good machine learning method is one with high accuracy and low false positive rate that can given new sets of data yield good results. 

Using supervised learning one can learn a model from a set of examples given predefined classes or training examples (Hvidsten, 2020). For the TCGA data this means that using a given algorithm one can create a model fitting to the TCGA dataset that needs to be analysed. Differentiating between some cancertypes may prove useful for cancers hard to differentiate physically. Such as colon or rectal cancer which both have the same symptoms (Fayed, 2020). For rectal cancer it is more important do discover it sooner as the cancer can easily spread to other tissues. This paper will implement different machine learning methods (KNN, SVM, Random Forest) to attempt to differentiate between rectum, colon, skin, testis and ovarian cancer.



## Methods

Data was extracted from TCGA using code in Appendix 1. The data contained RNA-Seq data from Skin, Ovary, Testis, Colon and Rectum cancer patients. The criteria for the datasets was that the sample had to have at least 20 million reads and genes had to have over 1024 mapped reads in at least 10 samples. From each cancer project, 100 samples was randomly selected. 
Principal component analysis (PCA) was performed using prcomp in the stats package. The two principal components were then visualized using the built in plot from base. PCA was performed to get an initial overview of the classes.

Differential expression (DE) analysis on the data types difficult to separate was performed using DESeq from the DEseq2 package (version 1.28.1). The significant genes was further normalized using DESeq2's VST-calculation. Significant genes was written into a csv file and the data from the significant genes were used for the rest of the analysis. 

Gene Ontology enrichment was performed using the csv file written from the significant gene data. The online tool Enrichr GO Biological Processes a list of enriched gene ontology terms was retrieved.   

Training data was constructed using the normalized feature selected data. 
Using the feature selected data, different machine learning methods were implemented.
KNN, SVM and the Random Forest method was analyzed both with and without cross validation to compare. 
KNN was performed using the class package from base.KNN testing was performed and the K with the highest accuracy was used to create a matrix table both with and without cross validation. 

SVM was performed using the package e1071 (version 1.7-4). A polynomial and radial SVM was performed and both were cross-validated. The Random Forest classification was performed using the randomForest package (version 4.6-14). Test data was further predicted using the predict function from the stats package. Cross validation was run using code in appendix 9


## Results

Code for the results can be seen in the appendix.

### Principal Component Analysis
![PCA](Pictures/PCA.png)  

*Figure 1 PCA of cancer data. Principal components only account for 34.25% of the deviation*

The PCA plot in figure 1 shows a good separation on Ovarian, testicular and skin cancer data, but struggles to separate rectum and colon as expected.  After PCA only colon and rectum data were analysed. Continuing on the colon and rectum data was analyzed after DESeq feature selection (See appendix 3)


### K- Nearest Neighbours
KNN testing was run to test if a linear function could separate the two cancer types.

![KNN-test](Pictures/KNN-test.png)  


*Figure 2 KNN testing with cross validation for most efficient KMER in separating colon and rectum data. Shows highest accuracy at 37. kmers.*

The KNN test shows that a K of 37 neighbours prooved to yield a highest accuracy in this test-data. Keep in mind that the dataset only has 200 possible kâ€™s so the boundary between the classes using linear method is not distinct at all. 



*Table 1 KNN table without cross validation for colon and rectum data after differential expression*


![KNN-test](Pictures/KNN.png) 


*Table 2 KNN table with cross validation showing predicted class types versus real*


![KNN-test](Pictures/KNN.cv.png) 

After running KNN without cross validation the initial observation from PCA plot is again observed. It appears to be difficult to separate rectum and colon data, whereas the other data is easier to separate. Without cross validation the accuracy was calculated to be 0.58, whereas with it was at 0.48. Which indicates that a linear model is not fitted for this dataset. Continuing with rectum vs colon cancer in comparisment, after cross validation the accuracy has decreased somewhat, but it is still difficult to separate rectum and colon.

### Support vector machine

*Table 3 SVM without cross validation with a polynomial kernel*

![SVM polynomial](Pictures/SVM.png) 


*Table 4 SVM modelling without cross validation using a sigmoid kernel*

![SVM sigmoid](Pictures/SVM-sigmoid.png) 

SVM without cross validation shows and accuracy of 0.57 for a polynomial kernel and 0.73 for a sigmoid kernel. After running a cross validation with 10 indices the error rate was calculated to be 0.58 for polynomial and 0.655 for the sigmoid kernel (see appendix 7). With this low accuracy these two SVM non-linear methods may not be ideal for separating colon/rectum data. 

### Random Forest


*Table 5 Confusion matrix and statistics showing Random Forest modelling of the Colon and Rectum cancer data*

![Random Forest Analysis](Pictures/Random-forest.png) 

The random forest classification method shows an accuracy of 0.64 with a 95% confidence interval between [0.4919, 0.7708]. Falling in the same accuracy rate of the other machine learning methods. With cross validation (1000 runs) however the accuracy had a mean of 0.46, falling below the confidence interval. Using OOB bootstrap implemented in the Random Forest a specificity of 0.4328 and sensitivity of 0.8696 was found with a detection rate of 0.3222. 

### GO analysis

![Significant genes and their supposed function](Pictures/GO-analysis.png)  

*Figure 3 GO analysis between the feature selected genes in Colon and Rectum dataset*
Figure 3 shows that the significant genes are prominent in the proteolysis part of the metabolic network. 


## Discussion

Using an PCA, an unsupervised method, it was proven rather quickly that skin, testis and ovarian cancer types are easily differentiable. However the principal components only accounted for 33.25% of the total variance meaning more analysis of these cancer types needs to be done. The focus was shifted toward colon and rectal cancer types as they proved to be the hardest to differentiate. 

The three different machine learning methods had a similar accuracy at around 0.64. Proving it difficult to classify using linear, and non-linear methods.After running every method with cross validation the accuracy remained more or less the same in SVM and KNN giving rise to a difficult classification. For the Random Forest method, accuracy seemed to decrease after cross validation from 0.54 to 0.46, which is still in the 95% confidence interval seen in table 5. 

The rectum is a part of the colon and it might be possible that rectum-cancer has been labelled as colon-cancer in this dataset. However seeing as they are so closely knit together in the PCA analysis there might also be anatomical reasons for this. According to the GO-analysis after sorting for significant genes using DESeq analysis, there are only 36 genes of significance. That is less that 0,08% of the total genes represented in this dataset. The GO analysis shows that the significant genes are prominent in proteolysis which is the first step in the utilization of protein in the human colon, and is in other words not so surprising. Another biological process discovered to be significant was the ion transport which is related to the human nervous system. The rectum has more nerve ending than the colon and the prominescence of such was again not surprising. GO-analysis using RNA-seq data did not give new information other than the biological differences between the colon and rectum. Making it seem as if the tumours are not differentiable from each other. 

Another machine learning method that could have been implemented to try and differentiate between the two cancer types would have been using Deeplearning. Deeplearning uses the training data to implement weights so that the output given is correct (Hvidsten, 2020). Given the biological similarity between these two cancertypes it would prove difficult to train this data without overfitting the dataset. However, a suggestion can be made for further analysis of the colon/rectum dataset and implement the deeplearning using cross validation.


## References


Fayed, L. *Differences Between Colorectal and Colon Cancer*, retrieved from verywellhealth (2020) read [19.11.20], https://www.verywellhealth.com/colon-cancer-and-colorectal-cancer-are-they-the-same-513949 

Gillard, GrÃ¸nvold & MÃ¤hler,  *BIN315: Lab 5- Supervised learning*, Canvas/BIN315-2020/Week-5 (2020).

Hvidsten, T.R. *Week 5 - BIN315 Machine Learning*, Canvas/BIN315-2020/Week-5 (2020)

Oehm, D. *Supervised Random Forest*, Gradient Descending (2018) retrieved 19.11.2020 from  http://gradientdescending.com/unsupervised-random-forest-example/ 

Lim, S.B., Tan, S.J., Lim, WT. et al. *Compendiums of cancer transcriptomes for machine learning applications.* Sci Data 6, 194 (2019). https://doi.org/10.1038/s41597-019-0207-2 

The National Cancer Institute, *The Cancer Genome Atlas Program* (No Year) retrieved 19.11.2020 from https://www.cancer.gov/about-nci/organization/ccg/research/structural-genomics/tcga

## Extra

### Appendix 1 data retrieval
Code retrieved from Gillard, GrÃ¸nvold & MÃ¤hler (2020)
```{r, eval=FALSE}  

if (!requireNamespace("BiocManager", quietly = TRUE))
  install.packages("BiocManager")
BiocManager::install("TCGAbiolinks")

suppressPackageStartupMessages(library(TCGAbiolinks))

# View all projects
# View(TCGAbiolinks:::getGDCprojects())

expr <- data.frame()
samples <- c()
classes <- c()

# Download RNA-Seq data for two selected projects
tissue <- c("Skin", "Testis", "Ovary", "Rectum", "Colon")
proj   <- c("TCGA-SKCM", "TCGA-TGCT", "TCGA-OV", "TCGA-READ", "TCGA-COAD")
for (i in 1:length(proj)) {

  query <- GDCquery(project = proj[i],
                    data.category = "Gene expression",
                    data.type = "Gene expression quantification",
                    platform = "Illumina HiSeq", 
                    file.type = "results",
                    experimental.strategy = "RNA-Seq",
                    legacy = TRUE)
  GDCdownload(query, method = "api", files.per.chunk = 10)
  
  data <- GDCprepare(query, )
  tmp.expr <- assay(data)
  tmp.samples <- paste(tissue[i], colData(data)$primary_diagnosis, sep="-")
  
  # Sample must have 20M reads
  idx <- colSums(tmp.expr) > 20E6
  tmp.expr <- tmp.expr[,idx]
  tmp.samples <- tmp.samples[idx]
  
  # Randomly select 100 samples
  idx <- sample(1:ncol(tmp.expr), 100)
  tmp.expr <- tmp.expr[,idx]
  tmp.samples <- tmp.samples[idx]
  
  if (nrow(expr) == 0) {
    expr <- tmp.expr
  } else {
    expr <- cbind(expr, tmp.expr)
  }
  samples <- c(samples, tmp.samples)
  classes <- c(classes, rep(tissue[i], length(tmp.samples)))
}
remove(tmp.expr, tmp.samples)
classes <- factor(classes)

table(classes)
dim(expr)

# Filtering
# Genes must have 1024 mapped reads in at least 10 samples
expr <- expr[rowSums(expr > 2^10) > 10,]
dim(expr)

save(expr, classes, file = "Project_TCGA.RData")
```

### Appendix 2 PCA 
Code retrieved from Gillard, GrÃ¸nvold & MÃ¤hler (2020)
```{r, results='hide',message=FALSE, warning=FALSE}
suppressPackageStartupMessages(library(beadarray))
suppressPackageStartupMessages(library(DESeq2))

load(file="Project_TCGA.RData")
load(file="Project_TCGA_Rectum_and_Colon.RData")

expr <- round(expr)

SampleTable <- data.frame(classes) # Saves column data for samples
colData <- data.frame(colnames(expr)) # Sets sample names to be colData 

dds <- DESeqDataSetFromMatrix(expr, colData = SampleTable, design = ~classes) 

# Sets sample name according to where it was retrieved
colnames(expr) <- classes

#Performs normalization
expr.norm <- varianceStabilizingTransformation(dds)

# Runs PCA
expr.pca <- prcomp(t(assay(expr.norm)))

#Plots PCA
plot(expr.pca$x[, 1], expr.pca$x[, 2], xlab = "PC1", ylab = "PC2", pch = 16, col = classes)
legend("topright", legend = levels(classes), pch = 16, col = 1:5)
```
Now this plot only accounts for 23 and 11.25% of the total variance, we might need to differentiate and filter significant genes in rectum and the colon, seeing as the other are differentiable. 

### Appendix 3 Feature Selection
Rectum and colon data are present as expr2 and classes2

```{r, results='hide',message=FALSE, warning=FALSE} 
suppressPackageStartupMessages(library(tidyverse))

# Need to remove some data

expr2 <- round(expr2)
SampleTable2 <- data.frame(classes2)

dds2 <- DESeqDataSetFromMatrix(expr2, colData = SampleTable2, design = ~classes2)


dds2 <- DESeq(dds2)
res <- results(dds2)
res0.05 <- subset(res, padj < 0.05)
res0.05[order(res0.05$padj),]

# Upregulated
sum(res0.05$log2FoldChange <= 0)

# Downregulated
sum(res0.05$log2FoldChange >= 0)

# False positive rate 
nrow(res0.05) * 0.05

# Filtration of FDR

# Lager deretter tabellen, deler opp i 2 da jeg slet med intervall
res_new <- na.omit(res)
res_1 <-  subset(res_new, res_new$log2FoldChange >= 1 & padj < 0.05)
res_2 <- subset(res_new, res_new$log2FoldChange <= -1 & padj < 0.05)

# SkjÃ¸ter sammen tabellene
res_n <- rbind(res_1, res_2)
significant_genes <- rownames(res_n)

feature_selected_data <- subset(dds2, rownames(dds2) %in% significant_genes)

feature_selected_data.norm <- varianceStabilizingTransformation(feature_selected_data)

```






### Appendix 4 Making training data
Code retrieved from Gillard, GrÃ¸nvold & MÃ¤hler (2020)
```{r, results='hide',message=FALSE, warning=FALSE}
suppressPackageStartupMessages(library(class))
suppressPackageStartupMessages(library(tidyverse))


expr <- t(assay(feature_selected_data.norm))
rownames(expr) <- classes2

set.seed(590)
test <- (assay(dds2))
colnames(test) <- classes2
random.test <- test[sample(nrow(test), 36), ]
row.random.test <- rownames(random.test)


Trclasses <- classes2 # Retrieves sample names for saving
Teclasses <- colnames(random.test)

classlabel <- c(classes2, colnames(random.test))


expr.unlabelled_training <- t(assay(feature_selected_data.norm)) # Fetches data and labels samples as null 
expr.unlabelled_test <- (assay(dds2))

colnames(expr.unlabelled_test) <- NULL
rownames(expr.unlabelled_training)<- NULL

# get 36 random rows


expr.train <- expr.unlabelled_training #Makes training data
expr.test <- subset(expr.unlabelled_test[, ], rownames(expr.unlabelled_test) %in% row.random.test)

expr.test <- t(expr.test)

expr.unlabelled_test <- t(expr.unlabelled_test)

expr.unlabelled <- cbind(expr.unlabelled_training, expr.unlabelled_test)






classlabel <-  str_replace_all(c("1" = "Colon", "2" = "Rectum"))


```

### Appendix 5 KNN 
Code retrieved from Gillard, GrÃ¸nvold & MÃ¤hler (2020)
```{r, results='hide',message=FALSE, warning=FALSE}

expr.knn <- knn(expr.train, expr.test, classes2, k = 36) # Runs KNN with 4 neighbors

confusion <- table(Teclasses, expr.knn, dnn = c("Real", "Predicted")) # Creates matrix table

# Measure accuracy of method
accuracy <- sum(diag(confusion))/ sum(confusion)

accuracy



expr.knn.cv <- knn.cv(expr.unlabelled, classes2, k=36) # Runs knn with cross validation

cv.confusion <- table(classes2, expr.knn.cv, dnn = c("Real", "Predicted"))

sum(diag(cv.confusion))/ sum(cv.confusion)
cv.confusion

```

### Appendix 6 Are any kmers more efficient?
Code retrieved from Gillard, GrÃ¸nvold & MÃ¤hler (2020)
```{r, results='hide',message=FALSE, warning=FALSE}
ks <- seq_len(50)
nrun <- 100
pred_accuracy <- c()
for (k in ks) {
  preds <- c()
  for (i in seq_len(nrun)) {
    k.pred <- knn.cv(expr.unlabelled, classes2, k = k)
    confusion <- table(k.pred, classes2)
    accuracy = sum(diag(confusion)) / sum(confusion)
    preds <- c(preds, accuracy)
  }
  pred_accuracy <- c(pred_accuracy, mean(preds))
}

plot(ks, pred_accuracy, type = "o", pch = 16, col = 1, ylab = "Mean accuracy", xlab = "k")
```

### Appendix 7 SVM
Code retrieved from Gillard, GrÃ¸nvold & MÃ¤hler (2020)

```{r, results='hide',message=FALSE, warning=FALSE}
suppressPackageStartupMessages(library(e1071))

# Trains model using training data
svm.model <- svm(expr.train, classes2[selected.rows$train], kernel = "polynomial")

# Predict classes for test data
svm.pred <- predict(svm.model, expr.test)

# Create a confusion matrix
confusion <- table(classes2[selected.rows$test], svm.pred, dnn = c("Real", "Predicted"))
confusion

# Measure accuracy of method
accuracy <- sum(diag(confusion))/ sum(confusion)

accuracy


## Now for training our SVM model:

# Start by getting the indices for all possible test sets
nsets <- 10 # The number of sets of different training/test data we want to use for cross validation
n <- nrow(expr.unlabelled) # The number of samples in the data
cv.sets <- split(sample(n), rep(seq_len(nsets), length = n)) # Create 10 different random sets of sample row numbers

accuracies <- NULL
# Iterate through the different sets of row numbers...
for (set in cv.sets) {
  # The current row numbers (`set`) corresponds to the test data, so
  # for training, we use `-set` to use all samples but the test samples.
  svm.model <- svm(expr.unlabelled[-set, ], kernel = "polynomial", classes2[-set])
  svm.pred <- predict(svm.model, expr.unlabelled[set, ])
  confusion <- table(classes2[set], svm.pred)
  # Calculate current accuracy, and add to the list of accuracies
  accuracies <- c(accuracies, sum(diag(confusion)) / sum(confusion)) 
}

# Calculate the mean accuracy from the 10 tests
mean_accuracy <- mean(accuracies)
mean_accuracy



```
```{r, results='hide',message=FALSE, warning=FALSE}
nsets <- 10 # The number of sets of different training/test data we want to use for cross validation
n <- nrow(expr.unlabelled) # The number of samples in the data
cv.sets <- split(sample(n), rep(seq_len(nsets), length = n)) # Create 10 different random sets of sample row numbers

accuracies <- NULL
# Iterate through the different sets of row numbers...
for (set in cv.sets) {
  # The current row numbers (`set`) corresponds to the test data, so
  # for training, we use `-set` to use all samples but the test samples.
  svm.model <- svm(expr.unlabelled[-set, ], kernel = "sigmoid", classes2[-set])
  svm.pred <- predict(svm.model, expr.unlabelled[set, ])
  confusion <- table(classes2[set], svm.pred)
  # Calculate current accuracy, and add to the list of accuracies
  accuracies <- c(accuracies, sum(diag(confusion)) / sum(confusion)) 
}

# Calculate the mean accuracy from the 10 tests
mean_accuracy <- mean(accuracies)
mean_accuracy
```


#### Grid search with SVM for data

Code retrieved from Gillard, GrÃ¸nvold & MÃ¤hler (2020)
```{r, results='hide',message=FALSE, warning=FALSE}
C <- c(0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, 100000)
gamma <- c(0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000)
kernel <- c("linear", "polynomial", "radial", "sigmoid")

# Get all combinations of C and gamma with the `expand.grid` function.
# It will return a data frame with the combinations, and that we then
# `cbind` with an empty accuracy vector that we will fill in the
# grid search.
acc.df <- cbind(expand.grid(C = C, gamma = gamma), accuracy = NA)

# For each C and gamma, find the accuracy of the model and add it to
# the data frame.
for (i in seq_len(nrow(acc.df))) {
  svm.model <- svm(expr.train, classes2[selected.rows$train], kernel = "sigmoid", cost = acc.df[i, "C"], gamma = acc.df[i, "gamma"])
  svm.pred <- predict(svm.model, expr.test)
  confusion <- table(classes2[selected.rows$test], svm.pred)
  acc.df$accuracy[i] <- sum(diag(confusion)) / sum(confusion)
}

# Plot contours of accuracies for different C and gamma values
library(ggplot2)
library(directlabels)
accuracy.contour <- ggplot(acc.df, aes(C, gamma, z = accuracy)) +
  stat_contour(aes(colour = ..level..), binwidth = 0.15) +
  scale_colour_gradient(low = "tomato", high = "forestgreen") +
  scale_x_log10("C") + scale_y_log10(bquote(gamma)) + theme_bw()
direct.label(accuracy.contour, "top.pieces")
``` 


### Appendix 8 Random Forest

code retrived from Oehm, no year
```{r, results='hide',message=FALSE, warning=FALSE}
# 

# create training data
set.seed(999)

holdout <- sample(1:150, 90)
data <- t(assay(feature_selected_data.norm))

suppressPackageStartupMessages(library(randomForest))
suppressPackageStartupMessages(library(caret))

data.rf1 <- randomForest(x = data[-holdout,-5], 
                         y = classes2[-holdout], 
                         mtry = 2, ntree = 600, 
                         proximity = TRUE)
rf


# Display Variable Importance Plot shows the most important variable
varImpPlot(data.rf1)

# Predict outcome for test set
y_predicted<-predict(data.rf1, expr.test)
df1<-data.frame(Orig=classes2[holdout], 
                Pred=y_predicted)

#Confusion matrix for test
confusionMatrix(table(df1$Orig, df1$Pred))

#dev.off()
plot(data.rf1, main="")
legend("topright", c("OOB", "Colon", "Rectum"), text.col=1:6, lty=1:3, col=1:3) 
title(main="Error Rates Random Forest Feature selected Training data")


#MDS on 1 - proximity
data.mds <- cmdscale(1-data.rf1$proximity, eig=TRUE)
op <- par(pty="s")
pairs(cbind(data[-holdout,-5], data.mds$points), cex=0.6, gap=0,
      col = c("red", "blue"), main= "TCGA data: predictors and mds of proximity based on randomforest")
par(op)
print(data.mds$GOF)
      


```
Optimale antall trÃ¦r ville vÃ¦rt der OOB er pÃ¥ sitt laveste - som er ved ca 600 trÃ¦r.

### Appendix 9 Cross validation of trees

Code inspired by Gillard, GrÃ¸nvold & MÃ¤hler (2020)
```{r, results='hide',message=FALSE, warning=FALSE}
counter <- 0
data <- t(assay(feature_selected_data.norm))
accuracies <- NULL


# Run until counter is 1000...
while (counter < 1000) {
  holdout <- sample(1:150, 90)
  rf.model <- randomForest(x = data[-holdout,-5], 
                         y = feature_selected_data.norm$classes2[-holdout], 
                         mtry = 2, ntree = 600, 
                         proximity = TRUE)
  
  y_predicted<-predict(rf.model, expr.test[holdout,-5])
  df1<-data.frame(Orig=classes2[holdout], 
                Pred=y_predicted)
  confusion <- table(df1$Orig, df1$Pred)
  # Calculate current accuracy, and add to the list of accuracies
  accuracies <- c(accuracies, sum(diag(confusion)) / sum(confusion)) 
  counter = counter+1
}

# Calculate the mean accuracy from the 10 tests
mean_accuracy <- mean(accuracies)
mean_accuracy
```






